{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb51f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erensmacbook/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import time\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPoint , Point\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Polygon, MultiPoint , Point\n",
    "from shapely import wkt\n",
    "import textgenrnn\n",
    "from textgenrnn import textgenrnn\n",
    "from scipy.sparse import random\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3be33179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def plot_dist(train_data_1,synthetic_data_cop):\n",
    "\n",
    "    dist_a = train_data_1.groupby('INDUSTRY').size()\n",
    "    dist_b = synthetic_data_cop.groupby('INDUSTRY').size()\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    index=dist_a.keys().to_list()\n",
    "\n",
    "    w=0.3\n",
    "    bar1=np.arange(len(index))\n",
    "    bar2= [i+w for i in bar1]\n",
    "\n",
    "    ax.bar(x=bar1 ,height=list(dist_a.values/dist_a.sum()), width=0.4, color='orange',label='real')\n",
    "    ax.bar(x=bar2 ,height=list(dist_b.values/dist_b.sum()), width=0.4, color='green',label='fake')\n",
    "\n",
    "    ax.autoscale(tight=True)\n",
    "    \n",
    "    ind=list(dist_a.keys())\n",
    "    plt.xticks(range(len(ind)),ind)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f25fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distance_c(train_data_1,syn_data,title):    \n",
    "    \n",
    "    anchor= train_data_1.distance.max()\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.xlim(0,round(anchor,3))\n",
    "\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Records')\n",
    "    plt.title(title)\n",
    "\n",
    "    sns.distplot(train_data_1['distance'],bins=30,hist_kws=dict(alpha=0.3),label='real',norm_hist=True)\n",
    "    sns.distplot(syn_data['distance'],bins=30,hist_kws=dict(alpha=0.3),label='fake',norm_hist=True,color='orange')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eafc036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_area_valid(real,fake,sector,title,thresh):\n",
    "    \n",
    "    hungariandest=fake.dest_sa2.unique()\n",
    "    realdest=real.dest_sa2.unique()\n",
    "    indices=np.intersect1d(hungariandest,realdest)\n",
    "    \n",
    "    hungarian_educ=[]\n",
    "    realdest_educ=[]\n",
    "    \n",
    "    ctgan_states = fake.groupby(['dest_sa2','INDUSTRY'])['INDUSTRY'].count()\n",
    "    # Change: groupby state_office and divide by sum\n",
    "    ctgan_pcts = ctgan_states.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "\n",
    "    for i in indices:\n",
    "        try:\n",
    "            hungarian_educ.append(ctgan_pcts[i][sector])\n",
    "        except KeyError:\n",
    "            hungarian_educ.append(0)\n",
    "        \n",
    "    for i in indices:\n",
    "        try:\n",
    "            realdest_educ.append(validation[validation.dest_sa2==i][sector][0]*100)\n",
    "        except KeyError:\n",
    "            realdest_educ.append(0)\n",
    "\n",
    "    sns.scatterplot( x=realdest_educ,  y=hungarian_educ)\n",
    "    plt.xlim(-1, thresh)\n",
    "    plt.ylim(-1, thresh)\n",
    "    xpoints = ypoints = plt.xlim()\n",
    "    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=3, scalex=True, scaley=True)\n",
    "    plt.xlabel(\"Validation percentages per SA2\")\n",
    "    plt.ylabel(\"Generated percentages per SA2\")\n",
    "    plt.title(title)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distance(syn_data):\n",
    "    syn_data = pd.merge(syn_data,qld_orig, on=\"ORIGSA1_2021\")\n",
    "    syn_data = pd.merge(syn_data,qld_dest, on=\"DESTSA1_2021\")\n",
    "    \n",
    "    syn_data['xo_c'] = syn_data[\"geo_o\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2)\n",
    "    syn_data['yo_c'] = syn_data[\"geo_o\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2)\n",
    "\n",
    "    syn_data['xd_c'] = syn_data[\"geo_d\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2)\n",
    "    syn_data['yd_c'] = syn_data[\"geo_d\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2)\n",
    "    \n",
    "    syn_data['orig_pt']=syn_data.apply(lambda x: Point((x['xo_c'],x['yo_c'])) ,axis=1 )\n",
    "    syn_data['dest_pt']=syn_data.apply(lambda x: Point((x['xd_c'],x['yd_c'])) ,axis=1 )\n",
    "    syn_data['distance']=syn_data.apply(lambda x: (x['orig_pt'].distance(x['dest_pt'])) ,axis=1 )\n",
    "    \n",
    "    return syn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83aa63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merger(df):\n",
    "    \n",
    "    merger=df.copy()\n",
    "    \n",
    "    merger['geo_0']=merger['0'].map(area_dict2)\n",
    "    merger['geo_1']=merger['1'].map(area_dict2)\n",
    "    merger['geo_2']=merger['2'].map(area_dict2)\n",
    "    merger['geo_3']=merger['3'].map(area_dict2)\n",
    "\n",
    "    merger = merger.dropna().reset_index()\n",
    "\n",
    "    merger['xo_0'] = merger[\"geo_0\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2 if x !=0 else 0 )\n",
    "    merger['yo_0'] = merger[\"geo_0\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2 if x !=0 else 0 )\n",
    "\n",
    "    merger['xo_1'] = merger[\"geo_1\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2 if x !=0 else 0 )\n",
    "    merger['yo_1'] = merger[\"geo_1\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2 if x !=0 else 0 )\n",
    "\n",
    "    merger['xo_2'] = merger[\"geo_2\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2 if x !=0 else 0 ) \n",
    "    merger['yo_2'] = merger[\"geo_2\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2 if x !=0 else 0 )\n",
    "\n",
    "    merger['xo_3'] = merger[\"geo_3\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2 if x !=0 else 0 )\n",
    "    merger['yo_3'] = merger[\"geo_3\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2 if x !=0 else 0 )\n",
    "\n",
    "    merger['pt_0']=merger.apply(lambda x: Point((x['xo_0'],x['yo_0'])) ,axis=1 )\n",
    "    merger['pt_1']=merger.apply(lambda x: Point((x['xo_1'],x['yo_1'])) ,axis=1 )\n",
    "    merger['pt_2']=merger.apply(lambda x: Point((x['xo_2'],x['yo_2'])) ,axis=1 )\n",
    "    merger['pt_3']=merger.apply(lambda x: Point((x['xo_3'],x['yo_3'])) ,axis=1 )\n",
    "\n",
    "    merger['distance01']=merger.apply(lambda x: 0 if x['pt_1'] ==  Point(0 ,0) else (x['pt_0'].distance(x['pt_1'])) ,axis=1 )\n",
    "    merger['distance02']=merger.apply(lambda x: 0 if x['pt_2'] ==  Point(0 ,0) else (x['pt_0'].distance(x['pt_2'])) ,axis=1 )\n",
    "    merger['distance03']=merger.apply(lambda x: 0 if x['pt_3'] ==  Point(0 ,0) else (x['pt_0'].distance(x['pt_3'])) ,axis=1 )\n",
    "    \n",
    "    return merger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_area(real,fake,sector,title,thresh):\n",
    "    hungariandest=fake.dest_sa2.unique()\n",
    "    realdest=real.dest_sa2.unique()\n",
    "    indices=np.intersect1d(hungariandest,realdest)\n",
    "    \n",
    "    hungarian_educ=[]\n",
    "    realdest_educ=[]\n",
    "    \n",
    "    ctgan_states = fake.groupby(['dest_sa2','INDUSTRY'])['INDUSTRY'].count()\n",
    "    # Change: groupby state_office and divide by sum\n",
    "    ctgan_pcts = ctgan_states.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "\n",
    "    for i in indices:\n",
    "        try:\n",
    "            hungarian_educ.append(ctgan_pcts[i][sector])\n",
    "        except KeyError:\n",
    "            hungarian_educ.append(0)\n",
    "        \n",
    "    for i in indices:\n",
    "        try:\n",
    "            realdest_educ.append(real_pcts[i][sector])\n",
    "        except KeyError:\n",
    "            realdest_educ.append(0)\n",
    "\n",
    "    sns.scatterplot( x=realdest_educ,  y=hungarian_educ)\n",
    "    plt.xlim(-1, thresh)\n",
    "    plt.ylim(-1, thresh)\n",
    "    xpoints = ypoints = plt.xlim()\n",
    "    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=3, scalex=False, scaley=False)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12120fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sa2(hungarian15k):\n",
    "    hungarian15k=hungarian15k.rename(columns={ \"DESTSA1_2021\":\"dest_sa\",\"ORIGSA1_2021\":\"orig_sa\"})\n",
    "\n",
    "\n",
    "    hungarian15k['SA1_CODE21']=hungarian15k['orig_sa']\n",
    "    hungarian15k=pd.merge(hungarian15k,sa1_sa2, on='SA1_CODE21')\n",
    "\n",
    "\n",
    "    hungarian15k=hungarian15k.rename(columns={\"SA2_CODE21\":\"orig_sa2\"})\n",
    "    hungarian15k = hungarian15k.drop(['SA1_CODE21'],axis=1)\n",
    "    hungarian15k['SA1_CODE21']=hungarian15k['dest_sa']\n",
    "\n",
    "    hungarian15k=pd.merge(hungarian15k,sa1_sa2, on='SA1_CODE21')\n",
    "    hungarian15k=hungarian15k.rename(columns={\"SA2_CODE21\":\"dest_sa2\"})\n",
    "    hungarian15k = hungarian15k.drop(['SA1_CODE21'],axis=1)\n",
    "    \n",
    "    return hungarian15k\n",
    "\n",
    "#plot_area(real_cat,hungarian50k,'health','10k kd tree education')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fde6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance(merger):\n",
    "    merg_dist= merger[['distance01','distance02','distance03']].idxmax(axis = 1)\n",
    "\n",
    "    distances =[]\n",
    "    sas=[]\n",
    "    for i in range(len(merger)):\n",
    "        indeks=merg_dist[i]\n",
    "\n",
    "        if indeks == 'distance01':\n",
    "            distances.append(merger['distance01'].iloc[i])\n",
    "            sas.append(merger['1'].iloc[i])\n",
    "\n",
    "        elif indeks =='distance02':\n",
    "            distances.append(merger['distance02'].iloc[i])\n",
    "            sas.append(merger['2'].iloc[i])\n",
    "    \n",
    "        elif indeks=='distance03':\n",
    "            distances.append(merger['distance03'].iloc[i])\n",
    "            sas.append(merger['3'].iloc[i])\n",
    "            \n",
    "            \n",
    "    merger['distance']=distances\n",
    "    merger['dist_sa']=sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b06a257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_toct(d1,ctgan_sample,rnn_sample):\n",
    "    dene = ctgan_sample.copy()\n",
    "    dene['indexes'] = list(range(0,len(rnn_sample)))\n",
    "    dene['indexes']=dene['indexes'].map(d1)\n",
    "    hungarian_indices=dene['indexes'].to_list()\n",
    "    ct_dests=[]\n",
    "    ct_origs=[]\n",
    "    for i in hungarian_indices:\n",
    "        ct_dests.append(rnn_sample['dest_sa'].iloc[i])\n",
    "        ct_origs.append(rnn_sample['0'].iloc[i])\n",
    "\n",
    "    dene['dest_sa'] = ct_dests\n",
    "    dene['orig_sa'] = ct_origs\n",
    "    \n",
    "    return dene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf052f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_dist(ctgan_sample,rnn_sample,ind):\n",
    "    \n",
    "    return (\n",
    "    ((ctgan_sample['xo_c'].iloc[ind] - rnn_sample['xo_0'])**2 ) + ( (ctgan_sample['yo_c'].iloc[ind] - rnn_sample['yo_0'])**2 )\n",
    "    ) ** (1/2)\n",
    "\n",
    "def create_frame(ctgan_sample,rnn_sample):\n",
    "    frame_list=[]\n",
    "    for i in range(len(ctgan_sample)): \n",
    "        a=euc_dist(ctgan_sample,rnn_sample,i).to_numpy()\n",
    "        frame_list.append(a)\n",
    "        #distance_matrix=pd.concat([distance_matrix,a],axis=1)\n",
    "        #if i%100 == 0:\n",
    "            #print(i)\n",
    "    frame_list=np.asarray(frame_list)\n",
    "\n",
    "    return frame_list\n",
    "\n",
    "### Ust indeksler, yani columns are ct, rowlar sol indeks rnn ler...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_max_ind(merger):\n",
    "    merg_dist= merger[['distance01','distance02','distance03']].idxmax(axis = 1)\n",
    "    merger['d_point'] = 0\n",
    "    merger['o_point'] = 0\n",
    "    merger['dest_sa'] = 0\n",
    "    \n",
    "    for i in range(len(merger)):\n",
    "        indeks=merg_dist[i]\n",
    "\n",
    "        if indeks == 'distance01':\n",
    "            merger['d_point'].iloc[i]= Point( (merger['xo_1'].iloc[i],merger['yo_1'].iloc[i]) )\n",
    "            merger['o_point'].iloc[i] =Point( (merger['xo_0'].iloc[i],merger['yo_0'].iloc[i]) )\n",
    "            merger['dest_sa'].iloc[i] = merger['1'].iloc[i]\n",
    "\n",
    "    \n",
    "        elif indeks =='distance02':\n",
    "            merger['d_point'].iloc[i]= Point( (merger['xo_2'].iloc[i],merger['yo_2'].iloc[i]) )\n",
    "            merger['o_point'].iloc[i] =Point( (merger['xo_0'].iloc[i],merger['yo_0'].iloc[i]) )\n",
    "            merger['dest_sa'].iloc[i] = merger['2'].iloc[i]\n",
    "\n",
    "\n",
    "            \n",
    "        elif indeks=='distance03':\n",
    "            merger['d_point'].iloc[i]= Point( (merger['xo_3'].iloc[i],merger['yo_3'].iloc[i]) )\n",
    "            merger['o_point'].iloc[i] =Point( (merger['xo_0'].iloc[i],merger['yo_0'].iloc[i]) )\n",
    "            merger['dest_sa'].iloc[i] = merger['3'].iloc[i]\n",
    "\n",
    "        if i%100==0:\n",
    "            print(str(i))\n",
    "                \n",
    "    return merger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d43731c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loads\n",
    "\n",
    "qts2=pd.read_excel('/Users/erensmacbook/Downloads/2qts.xlsx')\n",
    "qts5=pd.read_excel('/Users/erensmacbook/Downloads/5qts.xlsx')\n",
    "\n",
    "# Set filepath\n",
    "fp = \"/Users/erensmacbook/Downloads/SA1_2021_AUST_SHP_GDA2020/SA1_2021_AUST_GDA2020.shp\"\n",
    "\n",
    "# Read file using gpd.read_file()\n",
    "data = gpd.read_file(fp)\n",
    "\n",
    "##generating sa1-sa2 pairs to match them later on in result page\n",
    "sa1_sa2= data[['SA1_CODE21','SA2_CODE21']][0:-1]\n",
    "sa1_sa2['SA1_CODE21']=sa1_sa2['SA1_CODE21'].map(int)\n",
    "sa1_sa2['SA2_CODE21']=sa1_sa2['SA2_CODE21'].map(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2165c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process cnn\n",
    "\n",
    "qts2_t=qts2[['PERSID','AGEGROUP','SEX','WORKSTATUS','INDUSTRY']]\n",
    "qts2_t=qts2_t[(qts2_t.INDUSTRY.isna() != True)] ## Ogrenci bilgileri icin mainact ve studying later\n",
    "\n",
    "qts5_t=qts5[['PERSID','TRIPID',\n",
    "      'ORIGPLACE','ORIGSA1_2021'\n",
    "     ,'DESTPURP','DESTSA1_2021'\n",
    "     , 'DURATION','STARTIME']]\n",
    "\n",
    "real_data= pd.merge(qts2_t,qts5_t, on='PERSID')\n",
    "real_data\n",
    "\n",
    "\n",
    "data1= data[(data.SA4_NAME21=='Brisbane - East') |\n",
    "   (data.SA4_NAME21=='Brisbane - North')\n",
    "    |\n",
    "   (data.SA4_NAME21=='Brisbane - West')\n",
    "    |\n",
    "   (data.SA4_NAME21=='Brisbane Inner City')\n",
    "           |\n",
    "   (data.SA4_NAME21=='Brisbane - South')]\n",
    "\n",
    "qld_orig = data1[['SA1_CODE21','geometry']]\n",
    "qld_dest = data1[['SA1_CODE21','geometry']]\n",
    "\n",
    "qld_orig=qld_orig.rename(columns={\"SA1_CODE21\":\"ORIGSA1_2021\"})\n",
    "qld_dest=qld_dest.rename(columns={\"SA1_CODE21\":\"DESTSA1_2021\"})\n",
    "\n",
    "qld_orig['ORIGSA1_2021']=qld_orig['ORIGSA1_2021'].astype(int)\n",
    "qld_dest['DESTSA1_2021']=qld_dest['DESTSA1_2021'].astype(int)\n",
    "\n",
    "qld_orig=qld_orig.rename(columns={\"geometry\":\"geo_o\"})\n",
    "qld_dest=qld_dest.rename(columns={\"geometry\":\"geo_d\"})\n",
    "\n",
    "\n",
    "train_data = pd.merge(real_data,qld_orig, on=\"ORIGSA1_2021\")\n",
    "train_data= pd.merge(train_data,qld_dest, on=\"DESTSA1_2021\")\n",
    "\n",
    "train_data=train_data.sort_values(by=['PERSID', 'DURATION'])\n",
    "train_data= train_data.drop_duplicates(['PERSID'], keep='last').reset_index()\n",
    "\n",
    "train_data = train_data[['AGEGROUP', 'SEX','INDUSTRY','ORIGSA1_2021','DESTSA1_2021','geo_o','geo_d']]\n",
    "\n",
    "train_data['xo_c'] = train_data[\"geo_o\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2)\n",
    "train_data['yo_c'] = train_data[\"geo_o\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2)\n",
    "\n",
    "train_data['xd_c'] = train_data[\"geo_d\"].apply(lambda x: (x.bounds[0]+x.bounds[2]) /2)\n",
    "train_data['yd_c'] = train_data[\"geo_d\"].apply(lambda x: (x.bounds[1]+x.bounds[3]) /2)\n",
    "\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "train_data\n",
    "\n",
    "\n",
    "train_cat = train_data[['AGEGROUP','SEX','INDUSTRY','ORIGSA1_2021','DESTSA1_2021']]\n",
    "train_cord =train_data[['AGEGROUP','SEX','INDUSTRY','xo_c','yo_c','xd_c','yd_c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "271a6457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEGROUP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>INDUSTRY</th>\n",
       "      <th>ORIGSA1_2021</th>\n",
       "      <th>DESTSA1_2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>male</td>\n",
       "      <td>utilities</td>\n",
       "      <td>30403109418</td>\n",
       "      <td>30501110537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>male</td>\n",
       "      <td>construction</td>\n",
       "      <td>30101100407</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>female</td>\n",
       "      <td>professional</td>\n",
       "      <td>30103101601</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>male</td>\n",
       "      <td>professional</td>\n",
       "      <td>30103101601</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>female</td>\n",
       "      <td>wholesale</td>\n",
       "      <td>30303106525</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351</th>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>other</td>\n",
       "      <td>30103101401</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>9</td>\n",
       "      <td>female</td>\n",
       "      <td>realEstate</td>\n",
       "      <td>30103101505</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>12</td>\n",
       "      <td>female</td>\n",
       "      <td>education</td>\n",
       "      <td>30301105142</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5354</th>\n",
       "      <td>12</td>\n",
       "      <td>male</td>\n",
       "      <td>transport</td>\n",
       "      <td>30301105142</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>5</td>\n",
       "      <td>female</td>\n",
       "      <td>health</td>\n",
       "      <td>30103102031</td>\n",
       "      <td>30103101509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5356 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AGEGROUP     SEX      INDUSTRY  ORIGSA1_2021  DESTSA1_2021\n",
       "0           10    male     utilities   30403109418   30501110537\n",
       "1           14    male  construction   30101100407   30103101509\n",
       "2           11  female  professional   30103101601   30103101509\n",
       "3           11    male  professional   30103101601   30103101509\n",
       "4           13  female     wholesale   30303106525   30103101509\n",
       "...        ...     ...           ...           ...           ...\n",
       "5351         4    male         other   30103101401   30103101509\n",
       "5352         9  female    realEstate   30103101505   30103101509\n",
       "5353        12  female     education   30301105142   30103101509\n",
       "5354        12    male     transport   30301105142   30103101509\n",
       "5355         5  female        health   30103102031   30103101509\n",
       "\n",
       "[5356 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save proccessing for traiining\n",
    "train_cord.to_csv('/Users/erensmacbook/Desktop/train_cord_uniq.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ccfdd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (694453659.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/jv/2_fp3b717yl0vd8xbytqhz300000gn/T/ipykernel_32830/694453659.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ctgan train\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#ctgan train\n",
    "\n",
    "'''\n",
    "!pip install sdv\n",
    "\n",
    "from google.colab import files\n",
    "from sdv.constraints.base import Constraint, import_object\n",
    "from sdv.constraints import FixedCombinations\n",
    "from sdv.tabular import CTGAN\n",
    "\n",
    "df1= pd.read_csv(' SAVED DATA HERE ',index_col =None)\n",
    "from sdv.constraints.base import Constraint, import_object\n",
    "from sdv.constraints import FixedCombinations\n",
    "from sdv.tabular import CTGAN\n",
    "\n",
    "fixed_company_department_constraint = FixedCombinations(\n",
    "    column_names=['INDUSTRY','xo_c','yd_c'])\n",
    "\n",
    "types = {\n",
    "      'INDUSTRY' : { 'type': 'categorical'}  ,    \n",
    "      'xo_c' : { 'type': 'numerical', \"subtype\": \"float\"} ,\n",
    "      'yo_c' : { 'type': 'numerical', \"subtype\": \"float\"} ,\n",
    "      'xd_c' : { 'type': 'numerical', \"subtype\": \"float\"} ,\n",
    "      'yd_c' : { 'type': 'numerical', \"subtype\": \"float\"} }\n",
    "      \n",
    "\n",
    "ctgan5 = CTGAN(epochs=400,batch_size=20 ,constraints = [fixed_company_department_constraint],\n",
    "       field_types=types,discriminator_lr=0.01e-4 , generator_lr=0.01e-4 \n",
    "              , generator_dim=(32,32,32), discriminator_dim=(32, 32, 32)   )\n",
    "ctgan5.fit(df1)\n",
    "data5c = ctgan5.sample(50000)\n",
    "\n",
    "data5c.to_csv('ctgan50k.csv') \n",
    "files.download('ctgan50k.csv')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbd46fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_rnn = pd.merge(real_data,qld_orig, on=\"ORIGSA1_2021\")\n",
    "train_data_rnn = pd.merge(train_data_rnn,qld_dest, on=\"DESTSA1_2021\")\n",
    "train_data_rnn=train_data_rnn.sort_values(by=['PERSID', 'STARTIME'])\n",
    "cop=train_data_rnn.copy()\n",
    "\n",
    "act_ind=cop.PERSID.unique()\n",
    "\n",
    "group_traject=cop.groupby('PERSID')\n",
    "#group_traject.get_group(act_ind[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d11b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_of_a=[]\n",
    "industries=[]\n",
    "for i in act_ind:\n",
    "    temp=group_traject.get_group(i)\n",
    "    templist=temp.ORIGSA1_2021.to_list()\n",
    "    son=temp.DESTSA1_2021.to_list()[-1]\n",
    "    verdict=temp.INDUSTRY.values[0]\n",
    "    templist.append(son)\n",
    "    industries.append(verdict)\n",
    "\n",
    "    templist=list(map(str, templist)) \n",
    "    a_of_a.append(templist)\n",
    "\n",
    "act_df=pd.DataFrame.from_records(a_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd75c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "823b9fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' CODE IN TEXTGENRNN FOR RNN , COLAB FOR GPT CONDITIONAL'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RNN TRAIN\n",
    "\n",
    "''' CODE IN TEXTGENRNN FOR RNN , COLAB FOR GPT CONDITIONAL'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ad397cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frames=[]\\n\\nfor i in range(len(splitted_ct)):\\n    frame=create_frame(splitted_ct[i],splitted_rnn[i])\\n    frames.append(frame)\\n    print(str(i))\\n\\nprint(\\'ready\\')\\n## jıpyter issue?\\n\\nrowinds=[]\\ncolinds=[]\\ncount=0\\nfor i in frames:\\n    \\n    row_ind1, col_ind1 = linear_sum_assignment(i)\\n    rowinds.append(row_ind1)\\n    colinds.append(col_ind1)\\n    count = count + 1\\n    print(str(count))\\nprint(\"LSA1 solved\")\\n\\n\\nhung_dicts=[]\\nfor i in range(len(colinds)):\\n    d1=dict(zip(colinds[i],rowinds[i]))#for ct 1051, rnn 0 is best\\n    hung_dicts.append(d1)\\n\\n\\nassignments=[]\\nfor i in range(len(colinds)):\\n    assigned1=assign_toct(hung_dicts[i],splitted_ct[i],splitted_rnn[i])\\n    assignments.append(assigned1)\\n    \\n    \\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Matching CTGAN AND RNN RESULTS\n",
    "\n",
    "''' download the results from rnn and ctgan'''\n",
    "# rnn50k= pd.read_csv('/Users/erensmacbook/Downloads/rnn50k.csv',index_col=0)\n",
    "#ctgan_sample= pd.read_csv('/Users/erensmacbook/Downloads/ctgan50k.csv',index_col=0)\n",
    "\n",
    "#area_dict = dict(zip(qld_orig.ORIGSA1_2021, qld_orig.geo_o))\n",
    "#area_dict[0]=0\n",
    "#area_dict2 ={str(k):v for k,v in area_dict.items()}\n",
    "\n",
    "\n",
    "#generate_merger(rnn50k)\n",
    "#add_distance(rnn50k)\n",
    "\n",
    "\n",
    "\n",
    "### ready for hungarian matching\n",
    "# pd.concat([rnn_sample,rnn_sample2],ignore_index=True) ## concat ctgan and rnn if files are too big\n",
    "#splitted_ct=   np.array_split(ctgan_sample_170num, 20)\n",
    "#splitted_rnn=    np.array_split(rnn_sample, 20)   ## split population and trips for hungarian matching, max15k match per epoch is recommended\n",
    "\n",
    "\n",
    "## hungarianmatching\n",
    "\n",
    "#framelist1=create_frame(ctgan_sample,rnn)\n",
    "#row_ind1, col_ind1 = linear_sum_assignment(framelist1)\n",
    "#print(\"LSA1 solved\")\n",
    "\n",
    "#d1=dict(zip(col_ind1,row_ind1)) #keeep indices of best matches\n",
    "#assigned1=assign_toct(d1,ctgan_sample,rnn) ## assign values cnn and rnn\n",
    "\n",
    "\n",
    "''' if you have big population as chunks, matching routine for chunks here'''\n",
    "\n",
    "'''frames=[]\n",
    "\n",
    "for i in range(len(splitted_ct)):\n",
    "    frame=create_frame(splitted_ct[i],splitted_rnn[i])\n",
    "    frames.append(frame)\n",
    "    print(str(i))\n",
    "\n",
    "print('ready')\n",
    "## jıpyter issue?\n",
    "\n",
    "rowinds=[]\n",
    "colinds=[]\n",
    "count=0\n",
    "for i in frames:\n",
    "    \n",
    "    row_ind1, col_ind1 = linear_sum_assignment(i)\n",
    "    rowinds.append(row_ind1)\n",
    "    colinds.append(col_ind1)\n",
    "    count = count + 1\n",
    "    print(str(count))\n",
    "print(\"LSA1 solved\")\n",
    "\n",
    "\n",
    "hung_dicts=[]\n",
    "for i in range(len(colinds)):\n",
    "    d1=dict(zip(colinds[i],rowinds[i]))#for ct 1051, rnn 0 is best\n",
    "    hung_dicts.append(d1)\n",
    "\n",
    "\n",
    "assignments=[]\n",
    "for i in range(len(colinds)):\n",
    "    assigned1=assign_toct(hung_dicts[i],splitted_ct[i],splitted_rnn[i])\n",
    "    assignments.append(assigned1)\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "# assigned_all=pd.concat(assignments, ignore_index=True)\n",
    "# assigned_all.to_csv('/Users/erensmacbook/Downloads/hungarian200k.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c483f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### showing results plotting correlation etc...\n",
    "\n",
    "\n",
    "### after matching we need re-process the data \n",
    "\n",
    "'''hungariangpt=pd.read_csv('/Users/erensmacbook/Downloads/hungarian-gpt.csv')\n",
    "hungariangpt=hungariangpt.rename(columns={\"dest_sa\": \"DESTSA1_2021\", \"orig_sa\": \"ORIGSA1_2021\"})\n",
    "hungariangpt=generate_distance(hungariangpt)\n",
    "hungariangpt=add_sa2(hungariangpt) '''\n",
    "\n",
    "\n",
    "#plot_area_valid(validation,hungariangpt,'education','15k validation health',25)\n",
    "#plot_area(real_cat2,hungariangpt,'education','15k validation health',25)\n",
    "#plot_distance_c(real_cat,hungariangpt,'condrnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c796d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3536029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b929a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "602723b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validation=pd.read_csv(\\'/Users/erensmacbook/Desktop/validation2.csv\\',index_col=1)\\n#validation.rename(columns={\"Unnamed: 1\":\"ORIGSA1_2021\"})\\nvalidation=validation.rename(columns={\" \":\"SA2_CODE21\"})\\nvalidation = validation.iloc[1: , :]\\n\\ncol_list=validation.columns.to_list()\\ncol_list=col_list[1:-1]\\n\\ncol_list2=validation.columns.to_list()\\ncol_list2=col_list2[1:23]\\n\\nfor i in col_list2:\\n    validation[i] = validation[i].apply(lambda x:0 if x==\\' np \\' else x)\\n\\nfor i in col_list2:\\n    validation[i] = validation[i].astype(float)\\n\\nfor i in col_list:\\n    validation[i] = validation[i]/validation[\\'Total\\']\\n\\nvalidation[\\'SA2_CODE21\\']=validation[\\'SA2_CODE21\\'].map(int)\\nvalidation=validation.rename(columns={\"SA2_CODE21\":\"dest_sa2\"})'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### validation dataset process\n",
    "\n",
    "'''validation=pd.read_csv('/Users/erensmacbook/Desktop/validation2.csv',index_col=1)\n",
    "#validation.rename(columns={\"Unnamed: 1\":\"ORIGSA1_2021\"})\n",
    "validation=validation.rename(columns={\" \":\"SA2_CODE21\"})\n",
    "validation = validation.iloc[1: , :]\n",
    "\n",
    "col_list=validation.columns.to_list()\n",
    "col_list=col_list[1:-1]\n",
    "\n",
    "col_list2=validation.columns.to_list()\n",
    "col_list2=col_list2[1:23]\n",
    "\n",
    "for i in col_list2:\n",
    "    validation[i] = validation[i].apply(lambda x:0 if x==' np ' else x)\n",
    "\n",
    "for i in col_list2:\n",
    "    validation[i] = validation[i].astype(float)\n",
    "\n",
    "for i in col_list:\n",
    "    validation[i] = validation[i]/validation['Total']\n",
    "\n",
    "validation['SA2_CODE21']=validation['SA2_CODE21'].map(int)\n",
    "validation=validation.rename(columns={\"SA2_CODE21\":\"dest_sa2\"})'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879641df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### correlation analysis for diffferent methods\n",
    "\n",
    "'''\n",
    "sector='admin'\n",
    "hungariandest=hungarian200k.dest_sa2.unique()\n",
    "realdest=real_cat2.dest_sa2.unique()\n",
    "validdest=validation.dest_sa2.unique()\n",
    "#indices=np.intersect1d(hungariandest,validdest)\n",
    "indices=realdest\n",
    "\n",
    "hungarian_educ=[]\n",
    "realdest_educ=[]\n",
    "valid_educ=[]\n",
    "gpt_educ=[]\n",
    "before_educ=[]\n",
    "#ctgan_stringmatch=[]\n",
    "\n",
    "\n",
    "ctgan_states = hungarian200k.groupby(['dest_sa2','INDUSTRY'])['INDUSTRY'].count()\n",
    "    # Change: groupby state_office and divide by sum\n",
    "ctgan_pcts = ctgan_states.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "real_states = real_cat2.groupby(['dest_sa2','INDUSTRY'])['INDUSTRY'].count()\n",
    "    # Change: groupby state_office and divide by sum\n",
    "real_pcts = real_states.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "gpt_states = hungariangpt.groupby(['dest_sa2','INDUSTRY'])['INDUSTRY'].count()\n",
    "    # Change: groupby state_office and divide by sum\n",
    "gpt_pcts = gpt_states.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "before_states = ctgan_before.groupby(['dest_sa2','INDUSTRY'])['INDUSTRY'].count()\n",
    "    # Change: groupby state_office and divide by sum\n",
    "before_pcts = before_states.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "\n",
    "for i in indices:\n",
    "    try:\n",
    "        hungarian_educ.append(ctgan_states[i][sector])\n",
    "    except KeyError:\n",
    "        hungarian_educ.append(0)\n",
    "        \n",
    "for i in indices:\n",
    "    try:\n",
    "        realdest_educ.append(real_states[i][sector])\n",
    "    except KeyError:\n",
    "        realdest_educ.append(0)\n",
    "        \n",
    "for i in indices:\n",
    "    try:\n",
    "        #print(validation[validation.dest_sa2==i][sector][0])\n",
    "        if math.isnan(validation_n[validation_n.dest_sa2==i][sector][0]) == True:\n",
    "            valid_educ.append(0)\n",
    "        else:\n",
    "            valid_educ.append(validation_n[validation_n.dest_sa2==i][sector][0]*100)\n",
    "    except KeyError:\n",
    "        valid_educ.append(0)\n",
    "    except IndexError:\n",
    "        valid_educ.append(0)\n",
    "\n",
    "for i in indices:\n",
    "    try:\n",
    "        gpt_educ.append(gpt_states[i][sector])\n",
    "    except KeyError:\n",
    "        gpt_educ.append(0)\n",
    "        \n",
    "for i in indices:\n",
    "    try:\n",
    "        before_educ.append(before_states[i][sector])\n",
    "    except KeyError:\n",
    "        before_educ.append(0)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''df = pd.DataFrame()\n",
    "mySeries = pd.Series(realdest_educ) \n",
    "mySeries2 = pd.Series(hungarian_educ) \n",
    "mySeries3 = pd.Series(valid_educ) \n",
    "mySeries4 = pd.Series(gpt_educ) \n",
    "mySeries5 = pd.Series(before_educ) \n",
    "\n",
    "df['samp']=mySeries\n",
    "df['hungarian']=mySeries2\n",
    "df['valid']=mySeries3\n",
    "df['cond-rnn']=mySeries4\n",
    "df['ct-before']=mySeries5'''\n",
    "\n",
    "\n",
    "'''import pingouin as pg\n",
    "#pg.corr(x=df['A'], y=df['C'],method='spearman')\n",
    "pg.pairwise_corr(df,method='spearman') \n",
    "# import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import spatial\n",
    "\n",
    "## cosine similiartiy\n",
    "\n",
    "result = 1 - spatial.distance.cosine(mySeries, mySeries3)\n",
    "print('cosine similarity of valid vs sample: ' + str(result))\n",
    "\n",
    "result = 1 - spatial.distance.cosine(mySeries2, mySeries)\n",
    "print('cosine similarity of hungarian vs sample: ' + str(result))\n",
    "\n",
    "result = 1 - spatial.distance.cosine(mySeries2, mySeries3)\n",
    "print('cosine similarity of hungarian vs valid: ' + str(result))\n",
    "\n",
    "result = 1 - spatial.distance.cosine(mySeries4, mySeries)\n",
    "print('cosine similarity of cond vs sample: ' + str(result))\n",
    "\n",
    "result = 1 - spatial.distance.cosine(mySeries4, mySeries3)\n",
    "print('cosine similarity of cond vs valid: ' + str(result))\n",
    "\n",
    "result = 1 - spatial.distance.cosine(mySeries5, mySeries)\n",
    "print('cosine similarity of ct-before vs sample: ' + str(result))\n",
    "\n",
    "result = 1 - spatial.distance.cosine(mySeries5, mySeries3)\n",
    "print('cosine similarity of ct-before vs valid: ' + str(result)) '''\n",
    "\n",
    "\n",
    "### plotting correlation\n",
    "\n",
    "'''g = sns.JointGrid(data=df, x='samp', y='valid', height=5.5)\n",
    "g = g.plot_joint(sns.regplot, color=\"xkcd:muted blue\")\n",
    "g = g.plot_marginals(sns.distplot, kde=False, bins=10, color=\"xkcd:bluey grey\")\n",
    "#g.ax_joint.text(1,5,'r = 0.45, p < .001', fontstyle='italic')\n",
    "#plt.tight_layout()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
